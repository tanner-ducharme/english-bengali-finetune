{"cells":[{"cell_type":"markdown","source":["# fine tuning gemma-7b on english-bengali parallel corpus\n","this notebook allows you to run our group's most successful experiment (in terms of BLEU score achieved)\n","To run it, you'll have to use your own huggingface token and mount the notebook to where it exists in your google drive. everythin else should work as is\n","\n","### the first half of the notebook is for training the model\n","we exclusively used a V100 for training\n","### the second half is for generating predictions on the two test sets"],"metadata":{"id":"X3KeQshf9zjV"}},{"cell_type":"markdown","source":["#### install packages"],"metadata":{"id":"XHJ5-MKz97jy"}},{"cell_type":"code","source":["!pip install transformers trl datasets accelerate peft bitsandbytes"],"metadata":{"id":"A2VIQTdw93WI"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### paste HF token"],"metadata":{"id":"kRpCCMOA-jp5"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"TwSYoaDjoyWG"},"outputs":[],"source":["from huggingface_hub import notebook_login\n","# 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\n","\n","notebook_login()"]},{"cell_type":"markdown","source":["#### mount drive\n"],"metadata":{"id":"vt7hZ3uN-nTy"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"hlN2GjSSvifn"},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/drive/')\n","%cd /content/drive/MyDrive/path/to/your/folder/\n","\n","import sys\n","# If your Python files are in the 'part2' directory or a subdirectory of it, add 'part2' to the path\n","sys.path.append('/content/drive/MyDrive/path/to/your/folder/')"]},{"cell_type":"markdown","source":["#### this `exp_name` determines where the experimental results will be saved"],"metadata":{"id":"22Sr2Bdm-qeM"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"uuixVBKfV4Sx"},"outputs":[],"source":["exp_name = \"gemma-7b-test\""]},{"cell_type":"markdown","metadata":{"id":"aREsCz3Tu49W"},"source":["#### *Import* all the necessary packages."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"te74DElZ8r5t"},"outputs":[],"source":["from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, TrainingArguments\n","from peft import prepare_model_for_kbit_training, LoraConfig, get_peft_model\n","from datasets import load_dataset, concatenate_datasets\n","from trl import SFTTrainer\n","import torch"]},{"cell_type":"markdown","source":["#### establish configs for quantization, load model and load the tokenizer"],"metadata":{"id":"9qFsTYn1-0pv"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"lChdRaiR81Dc"},"outputs":[],"source":["model_name = \"google/gemma-7b\"\n","\n","compute_dtype = getattr(torch, \"float16\")\n","bnb_config = BitsAndBytesConfig(load_in_4bit=True,\n","            bnb_4bit_quant_type=\"nf4\",\n","            bnb_4bit_compute_dtype=compute_dtype,\n","            bnb_4bit_use_double_quant=True,\n",")\n","model = AutoModelForCausalLM.from_pretrained(model_name,quantization_config=bnb_config, device_map={\"\": 0})\n","model = prepare_model_for_kbit_training(model)\n","\n","tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True, add_eos_token=True)\n","tokenizer.pad_token = tokenizer.unk_token\n","tokenizer.padding_side = \"left\""]},{"cell_type":"markdown","source":["#### read training data from HF"],"metadata":{"id":"PSCq0f6n-9jA"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"BE7djCZ2_Qdf"},"outputs":[],"source":["raw_data = load_dataset(\"csebuetnlp/BanglaNMT\")"]},{"cell_type":"markdown","source":["#### get subset of data since we don't have the resources to train on the entire *dataset*\n","\n","partition training and validation data"],"metadata":{"id":"05C40m0K_A8z"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"NPqFLRORE5pG"},"outputs":[],"source":["# get 100_000 elements from raw_data\n","raw_data_train = raw_data['train'].select(range(10000)).shuffle(seed=42)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hWeqokjGGMre"},"outputs":[],"source":["raw_data_valid = raw_data['validation']"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ybM5pEI7RTcz"},"outputs":[],"source":["def generate_prompt(data_point, instruction, source, target):\n","    \"\"\"Gen. input text based on a prompt, task instruction, (context info.), and answer\n","\n","    :param data_point: dict: Data point\n","    :return: dict: tokenzed prompt\n","    \"\"\"\n","\n","    text = f\"\"\"<start_of_turn>user\n","    {instruction}{data_point[source]}<end_of_turn>\n","    <start_of_turn>model\n","    {data_point[target]} <end_of_turn>\n","    \"\"\"\n","    return text\n"]},{"cell_type":"markdown","source":["#### code for adding bidirectional-translation data\n","this code adds examples of english to bengali data points and bengali to english datapoints to our training data <br>\n","### the hope is that the bidirectional data better allows the model to learn the relationship between the two languages"],"metadata":{"id":"5tA1_vxJ_XTU"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"7jcVadzeNvZ7"},"outputs":[],"source":["\n","instruction = \"Translate the following Bengali text to English: \"\n","\n","train_dataset_bn_en = raw_data_train.map(lambda example: {'prompt':f\"\"\"<start_of_turn>user\n","{instruction}{example['bn']}<end_of_turn>\n","<start_of_turn>model\n","{example['en']} <end_of_turn>\"\"\"})\n","\n","instruction = \"Translate the following English text to Bengali: \"\n","\n","train_dataset_en_bn = raw_data_train.map(lambda example: {'prompt':f\"\"\"<start_of_turn>user\n","{instruction}{example['en']}<end_of_turn>\n","<start_of_turn>model\n","{example['bn']} <end_of_turn>\"\"\"})"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"n4F7GSrKG4pJ"},"outputs":[],"source":["combined_train_dataset = concatenate_datasets([train_dataset_bn_en, train_dataset_en_bn])\n","combined_train_dataset = combined_train_dataset.shuffle(seed=42)"]},{"cell_type":"markdown","source":["pre-process validation data"],"metadata":{"id":"09GxRb8E_wO3"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"l9PxjLOmImhr"},"outputs":[],"source":["instruction = \"Translate the following Bengali text to English: \"\n","valid_dataset = raw_data_valid.map(lambda example: {'prompt':f\"\"\"<start_of_turn>user\n","{instruction}{example['bn']}<end_of_turn>\n","<start_of_turn>model\n","{example['en']} <end_of_turn>\"\"\"})"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dqumn8oxI2-0"},"outputs":[],"source":["valid_dataset['prompt'][0]"]},{"cell_type":"markdown","metadata":{"id":"AN6REayvYqb1"},"source":["LoRA configuration:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jmVQKkMI9AZQ"},"outputs":[],"source":["peft_config = LoraConfig(\n","            lora_alpha=16,\n","            lora_dropout=0.05,\n","            r=16,\n","            bias=\"none\",\n","            task_type=\"CAUSAL_LM\",\n","            target_modules= [\"down_proj\",\"up_proj\",\"gate_proj\"]\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zP0niVJUXOJX"},"outputs":[],"source":["torch.cuda.empty_cache()"]},{"cell_type":"markdown","source":["#### establish training arguments\n","this will save checkpoints in a \"/results/exp_name/\" folder"],"metadata":{"id":"naN62w3L_5i_"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"cxc8sr_4-7DM"},"outputs":[],"source":["training_arguments = TrainingArguments(\n","        output_dir=f\"/results/{exp_name}\",\n","        evaluation_strategy=\"steps\",\n","        optim=\"paged_adamw_8bit\",\n","        save_steps=500,\n","        log_level=\"debug\",\n","        logging_steps=20,\n","        learning_rate=2e-5,\n","        eval_steps=100,\n","        fp16=True,\n","        do_eval=True,\n","        auto_find_batch_size=True,\n","        warmup_steps=100,\n","        max_steps=1000,\n","        lr_scheduler_type=\"linear\"\n",")"]},{"cell_type":"markdown","source":["#### create trainer"],"metadata":{"id":"wX50MFZDASkj"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"bu-_d4YP_CkR"},"outputs":[],"source":["trainer = SFTTrainer(\n","        model=model,\n","        train_dataset=combined_train_dataset,\n","        eval_dataset=valid_dataset,\n","        peft_config=peft_config,\n","        dataset_text_field=\"prompt\",\n","        max_seq_length=256,\n","        tokenizer=tokenizer,\n","        args=training_arguments\n",")\n"]},{"cell_type":"markdown","source":["#### train"],"metadata":{"id":"GP-ODM-QAVp4"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"fPmdfWjMXVuf"},"outputs":[],"source":["trainer.train()"]},{"cell_type":"markdown","source":["# important\n","the remainder of the code is for loading the fine-tuned adapter weights, merging it with the baseline model, and generating predictions<br>\n","sometimes it is necessary to restart the kernel (for GPU reasons) in order to run this code. **If you are restarting the kernel, just make sure you re-mount your drive at the top.** Then the rest of this code can be executed"],"metadata":{"id":"ZZrj3oDjAZGF"}},{"cell_type":"markdown","metadata":{"id":"CO-evmFuoyl9"},"source":["#### install packages"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"e60mJrsLE-Gz"},"outputs":[],"source":["!pip install transformers accelerate peft bitsandbytes datasets"]},{"cell_type":"markdown","source":["#### import libraries"],"metadata":{"id":"FemrfKOXA32p"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"9WIgCEFmFDDv"},"outputs":[],"source":["from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, GenerationConfig\n","import torch\n","from peft import PeftModel"]},{"cell_type":"markdown","source":["make sure this is the same as the one used in the first half of the notebook"],"metadata":{"id":"DYqmmmFHA53M"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"qVIAIhOhaD8M"},"outputs":[],"source":["exp_name = \"gemma-7b-test\""]},{"cell_type":"markdown","metadata":{"id":"kgxRguzyo2HT"},"source":["#### re-authenticate (if necessary)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HQaKer_qF5s-"},"outputs":[],"source":["from huggingface_hub import notebook_login\n","\n","# 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\n","\n","notebook_login()"]},{"cell_type":"markdown","source":["#### use the same parameters as before for loading and quantizing the base model"],"metadata":{"id":"UQWPmrSZBHnm"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"bo_gMGfDFGzQ"},"outputs":[],"source":["base_model = \"google/gemma-7b\"\n","compute_dtype = getattr(torch, \"float16\")\n","bnb_config = BitsAndBytesConfig(\n","    load_in_4bit=True,\n","    bnb_4bit_quant_type=\"nf4\",\n","    bnb_4bit_compute_dtype=compute_dtype,\n","    bnb_4bit_use_double_quant=True,\n",")\n","model = AutoModelForCausalLM.from_pretrained(\n","        base_model, device_map={\"\": 0}, quantization_config=bnb_config\n",")\n","tokenizer = AutoTokenizer.from_pretrained(base_model, use_fast=True)"]},{"cell_type":"markdown","metadata":{"id":"WTmpRQ3-HuRp"},"source":["#### load test data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CfMQyivxIow2"},"outputs":[],"source":["def read_sentences(file_path):\n","  with open(file_path, encoding='utf-8') as file:\n","      sentences = file.read().strip().split('\\n')\n","  return sentences"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iWiMbfoNIz5e"},"outputs":[],"source":["def generate_eval_prompt(data_point, instruction, source):\n","    \"\"\"Gen. input text based on a prompt, task instruction, (context info.), and answer\n","\n","    :param data_point: dict: Data point\n","    :return: dict: tokenzed prompt\n","    \"\"\"\n","    # print(instruction)\n","    # print(data_point)\n","    # print(source)\n","\n","    text = f\"\"\"<start_of_turn>user\n","{instruction}{data_point}<end_of_turn>\n","<start_of_turn>model\"\"\"\n","    return text\n"]},{"cell_type":"markdown","source":["### important:\n","make sure these files paths are correct for your drive set up (they should be good to go)"],"metadata":{"id":"9d3NU_jIBRqf"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"pfcuVBM-IfUj"},"outputs":[],"source":["source_lang = \"bn\"\n","target_lang = \"en\"\n","\n","\n","# get test data\n","supara_source_val_path = f\"data/SUPara-benchmark/suparadev2018/suparadev_{source_lang}.txt\"\n","supara_target_val_path = f\"data/SUPara-benchmark/suparadev2018/suparadev_{target_lang}.txt\"\n","\n","rising_source_val_path = f\"data/RisingNews-benchmark/RisingNews.valid.{source_lang}\"\n","rising_target_val_path = f\"data/RisingNews-benchmark/RisingNews.valid.{target_lang}\"\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DfdV_4h7Ifey"},"outputs":[],"source":["# # read SUPara source language data\n","supara_source_val_raw = read_sentences(supara_source_val_path)\n","# # read SUPara target language data\n","supara_target_val = read_sentences(supara_target_val_path)\n","\n","# # read SUPara source language data\n","rising_source_val_raw = read_sentences(rising_source_val_path)\n","# # read SUPara target language data\n","rising_target_val = read_sentences(rising_target_val_path)"]},{"cell_type":"markdown","source":["#### prepend instructions and format test data into correct format"],"metadata":{"id":"eng0IkIsBcHn"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"Y7OxwZCwcu2x"},"outputs":[],"source":["instruction = \"Translate the following Bengali text into English:\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EKt6XvCmIoRy"},"outputs":[],"source":["supara_source_val = [generate_eval_prompt(example, instruction, 'bn') for example in supara_source_val_raw]\n","rising_source_val = [generate_eval_prompt(example, instruction, 'bn') for example in rising_source_val_raw]\n","\n","\n","print(\"SUPara source validation sentence before and after system prompt:\")\n","print(f\"BEFORE:\\n{supara_source_val_raw[0]}\")\n","print(f\"AFTER:\\n{supara_source_val[0]}\")\n","\n","print(\"\\nSUPara target language test sentence:\")\n","print(supara_target_val[0])\n","\n","print(\"\\nRisingNews source validation sentence before and after system prompt:\")\n","print(f\"BEFORE:\\n{rising_source_val_raw[0]}\")\n","print(f\"AFTER:\\n{rising_source_val[0]}\")\n","\n","\n","\n","print(\"\\nRisingNews target language test sentence:\")\n","print(rising_target_val[0])"]},{"cell_type":"markdown","source":["#### convert test data in to HF Dataset object"],"metadata":{"id":"4u-R9ii_Bhf5"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"fSr4c8pxJHbO"},"outputs":[],"source":["from datasets import Dataset\n","import pandas as pd\n","\n","supara_df = pd.DataFrame({'source': supara_source_val, 'target': supara_target_val})\n","rising_df = pd.DataFrame({'source': rising_source_val, 'target': rising_target_val})\n","\n","\n","supara_dataset = Dataset.from_pandas(supara_df)\n","rising_dataset = Dataset.from_pandas(rising_df)\n","rising_dataset['source'][0]"]},{"cell_type":"markdown","metadata":{"id":"vYJOSMuWo-Zo"},"source":["#### load checkpoints <br>\n","for the best version of our experiment, we ran for 10000 iterations  <br>\n","since this notebook is just a proof-of-concept, we have it set to run for only 1000"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4DZRkm1sFLBv"},"outputs":[],"source":["full_model = PeftModel.from_pretrained(model, f\"results/{exp_name}/checkpoint-1000/\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"C0HsHhyvZfn3"},"outputs":[],"source":["del model"]},{"cell_type":"markdown","source":[],"metadata":{"id":"e39trAqgB3VG"}},{"cell_type":"markdown","source":["#### code for generating predictions\n","this code takes a long time to run <br>\n","in case of colab timeouts, I wrote it so that it saves every 100 predictions to a csv to prevent data loss"],"metadata":{"id":"pyJUElHDB3W4"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"N7vO2QuMMCAV"},"outputs":[],"source":["from tqdm import tqdm\n","import pandas as pd\n","import torch\n","import re\n","\n","num_predictions = len(rising_dataset['source'])\n","\n","save_every_n = 100\n","\n","# Assuming rising_dataset, tokenizer, and full_model are already defined\n","predictions = []\n","\n","# Function to save dataframe\n","def save_dataframe(index):\n","    # Load existing data if it exists\n","    print(f\"Saving dataframe for index {index}\")\n","    try:\n","        df_existing = pd.read_csv(f\"results/{exp_name}/gemma_7b_rising_preds.csv\")\n","    except FileNotFoundError:\n","        df_existing = pd.DataFrame(columns=['source', 'target', 'prediction'])\n","\n","    # Create new dataframe from current predictions\n","    df_new = pd.DataFrame({\n","        'source': rising_dataset['source'][index-save_every_n:index],\n","        'target': rising_dataset['target'][index-save_every_n:index],\n","        'prediction': predictions[-save_every_n:]\n","    })\n","\n","    # Append new data and save\n","    df_combined = pd.concat([df_existing, df_new], ignore_index=True)\n","    df_combined.to_csv(f\"results/{exp_name}/gemma_7b_rising_preds.csv\", index=False)\n","\n","# Adjust for your needs\n","\n","torch.cuda.empty_cache()\n","for i, source in enumerate(tqdm(rising_dataset['source'][:num_predictions])):\n","    torch.cuda.empty_cache()\n","    tokenized_input = tokenizer(source, return_tensors=\"pt\")\n","    input_ids = tokenized_input[\"input_ids\"].cuda()\n","    del tokenized_input\n","    generation_output = full_model.generate(\n","        input_ids=input_ids,\n","        num_beams=6,\n","        return_dict_in_generate=True,\n","        output_scores=True,\n","        max_new_tokens=130\n","    )\n","    del input_ids\n","    for seq in generation_output.sequences:\n","        output = tokenizer.decode(seq, skip_special_tokens=True)\n","        pattern = r'Translate the following Bengali text into English:.*?\\nmodel\\n(.*)'\n","        match = re.search(pattern, output, re.DOTALL)  # re.DOTALL allows '.' to match newlines as well\n","\n","        if match:\n","            prediction = match.group(1).strip()  # .strip() to remove any leading or trailing whitespace\n","            print(\"Prediction:\", prediction)\n","        else:\n","            print(\"No match found.\")\n","\n","\n","        predictions.append(prediction)\n","\n","    # Save every 100 predictions\n","    if (i + 1) % save_every_n == 0:\n","        save_dataframe(i + 1)\n","        print(f\"\\nSOURCE:\\n{source}\")\n","        print(f\"PREDICTION:\\n{prediction}\")\n","        print(f\"TARGET:\\n{rising_dataset['target'][i]}\\n\\n\")\n","\n","torch.cuda.empty_cache()\n","\n","# Save remaining predictions if there are any\n","if len(predictions) % 100 > 0:\n","    save_dataframe(len(predictions))\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"q5_-kl7-RtnH"},"outputs":[],"source":["from tqdm import tqdm\n","import pandas as pd\n","import torch\n","\n","# Adjust for your needs\n","num_predictions = len(supara_dataset['source'])\n","\n","# Assuming rising_dataset, tokenizer, and full_model are already defined\n","predictions = []\n","\n","# Function to save dataframe\n","def save_dataframe(index):\n","    # Load existing data if it exists\n","    print(f\"Saving dataframe for index {index}\")\n","    try:\n","        df_existing = pd.read_csv(f\"results/{exp_name}/gemma_7b_supara_preds.csv\")\n","    except FileNotFoundError:\n","        df_existing = pd.DataFrame(columns=['source', 'target', 'prediction'])\n","\n","    # Create new dataframe from current predictions\n","    df_new = pd.DataFrame({\n","        'source': supara_dataset['source'][index-100:index],\n","        'target': supara_dataset['target'][index-100:index],\n","        'prediction': predictions[-100:]\n","    })\n","\n","    # Append new data and save\n","    df_combined = pd.concat([df_existing, df_new], ignore_index=True)\n","    df_combined.to_csv(f\"results/{exp_name}/gemma_7b_supara_preds.csv\", index=False)\n","\n","\n","\n","torch.cuda.empty_cache()\n","for i, source in enumerate(tqdm(supara_dataset['source'][:num_predictions])):\n","    torch.cuda.empty_cache()\n","    tokenized_input = tokenizer(source, return_tensors=\"pt\")\n","    input_ids = tokenized_input[\"input_ids\"].cuda()\n","    del tokenized_input\n","    generation_output = full_model.generate(\n","        input_ids=input_ids,\n","        num_beams=6,\n","        return_dict_in_generate=True,\n","        output_scores=True,\n","        max_new_tokens=130\n","    )\n","    del input_ids\n","    for seq in generation_output.sequences:\n","        output = tokenizer.decode(seq, skip_special_tokens=True)\n","        pattern = r'Translate the following Bengali text into English:.*?\\nmodel\\n(.*)'\n","        match = re.search(pattern, output, re.DOTALL)  # re.DOTALL allows '.' to match newlines as well\n","\n","        if match:\n","            prediction = match.group(1).strip()  # .strip() to remove any leading or trailing whitespace\n","            print(\"Prediction:\", prediction)\n","        else:\n","            print(\"No match found.\")\n","\n","\n","        predictions.append(prediction)\n","\n","    # Save every 100 predictions\n","    if (i + 1) % 100 == 0:\n","        save_dataframe(i + 1)\n","        print(f\"\\nSOURCE:\\n{source}\")\n","        print(f\"PREDICTION:\\n{prediction}\")\n","        print(f\"TARGET:\\n{supara_dataset['target'][i]}\\n\\n\")\n","\n","torch.cuda.empty_cache()\n","\n","# Save remaining predictions if there are any\n","if len(predictions) % 100 > 0:\n","    save_dataframe(len(predictions))\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GSTtgA7xSV-8"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nnDmQXR0SWBn"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9aSxiETHLU7J"},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"V100","provenance":[{"file_id":"1-Cg7L4yzCspmRjzJk_vP8G1m4SHNRwB4","timestamp":1712182032974},{"file_id":"1OpyEzzmzgkWEHO3hsSmuyb82cdBWfV3u","timestamp":1698776733409}]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}